{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT\n",
    "from medclip.modeling_medclip import MedCLIPVisionModel\n",
    "from medclip import MedCLIPProcessor\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88263d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for the demo image and texts\n",
    "processor = MedCLIPProcessor()\n",
    "\n",
    "image = Image.open('./example_data/view1_frontal.jpg')\n",
    "inputs = processor(\n",
    "    text=[\"lungs remain severely hyperinflated with upper lobe emphysema\", \n",
    "        \"opacity left costophrenic angle is new since prior exam ___ represent some loculated fluid cavitation unlikely\"], \n",
    "    images=image, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18190d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model from pretrained checkpoint\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb39f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ccc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d81293",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_dataset, LESION_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medclip = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "\n",
    "def clip_preprocess(image):\n",
    "    return processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "ham_train, ham_test = load_dataset(\"HAM10000\", transform=clip_preprocess)\n",
    "print(f\"Train size: {len(ham_train)}\")\n",
    "print(f\"Test size: {len(ham_test)}\")\n",
    "print(ham_train)\n",
    "print(ham_test)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a zero-shot function for medclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a zero-shot function for medclip\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from build.lib.medclip.constants import BERT_TYPE, IMG_MEAN, IMG_STD, IMG_SIZE\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from data_utils import load_ham10000_dataset, LESION_TYPE\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def medclip_zero_shot_inline(test_dataset, classes, batch_size=BATCH_SIZE):\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Data loader for the dataset\n",
    "    data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Initialize MedClip Models\n",
    "    model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT).to(device)\n",
    "\n",
    "    # Prepare text prompts\n",
    "    text_prompts = [f\"a photo of a {c}, a type of skin lesion.\" for c in classes]\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "    # Tokenize text prompts and convert to tensors\n",
    "    text_tokens = [tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True) for text in text_prompts]\n",
    "\n",
    "    # Encode text prompts using MedClip's text model\n",
    "    # Inside the medclip_zero_shot function\n",
    "    text_features = [\n",
    "        model.encode_text(\n",
    "            input_ids=tokens['input_ids'].to(device), \n",
    "            attention_mask=tokens['attention_mask'].to(device)\n",
    "        ) \n",
    "        for tokens in text_tokens\n",
    "    ]\n",
    "\n",
    "    # Initialize variables for accuracy calculation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # TODO: Encode images using MedClip's vision model\n",
    "        image_features = model.encode_image(images)\n",
    "\n",
    "        # Flatten text_features into a single 2D tensor\n",
    "        text_features_tensor = torch.cat(text_features, dim=0)\n",
    "\n",
    "        # Calculate similarity and make predictions\n",
    "        similarity = torch.matmul(image_features, text_features_tensor.t())\n",
    "        _, predictions = similarity.max(dim=-1)\n",
    "\n",
    "        # Update correct and total counts\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Load HAM10000 dataset\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[IMG_MEAN], std=[IMG_STD])\n",
    "])\n",
    "\n",
    "train_dataset, test_dataset = load_ham10000_dataset(data_dir=\"data/ham10000/\", transform=transform)\n",
    "classes = list(LESION_TYPE.values())  # From the data_utils.py file\n",
    "\n",
    "# Run zero-shot classification\n",
    "acc = medclip_zero_shot_inline(test_dataset, classes)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medclip_zero_shot(model, test_dataset, classes, batch_size=BATCH_SIZE):\n",
    "    # Data loader for the dataset\n",
    "    data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Prepare text prompts\n",
    "    text_prompts = [f\"a photo of a {c}, a type of skin lesion.\" for c in classes]\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Tokenize text prompts and convert to tensors\n",
    "    text_tokens = [tokenizer(text, return_tensors='pt', padding=True, truncation=False, add_special_tokens=True) for text in text_prompts]\n",
    "\n",
    "    # Encode text prompts using MedClip's text model\n",
    "    # Inside the medclip_zero_shot function\n",
    "    text_features = [\n",
    "        model.encode_text(\n",
    "            input_ids=tokens['input_ids'].to(device), \n",
    "            attention_mask=tokens['attention_mask'].to(device)\n",
    "        ) \n",
    "        for tokens in text_tokens\n",
    "    ]\n",
    "\n",
    "    # Initialize variables for accuracy calculation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Encode images using MedClip's vision model\n",
    "        # with torch.no_grad():\n",
    "        image_features = model.encode_image(images)\n",
    "        # Flatten text_features into a single 2D tensor\n",
    "        text_features_tensor = torch.cat(text_features, dim=0)\n",
    "\n",
    "        # Calculate similarity and make predictions\n",
    "        similarity = torch.matmul(image_features, text_features_tensor.t())\n",
    "        _, predictions = similarity.max(dim=-1)\n",
    "\n",
    "        # Update correct and total counts\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HAM10000 dataset and test MedClip's zero-shot capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[IMG_MEAN], std=[IMG_STD])\n",
    "])\n",
    "\n",
    "ham_train, ham_test = load_ham10000_dataset(data_dir=\"data/ham10000/\", transform=transform)\n",
    "classes = list(LESION_TYPE.values())  # From the data_utils.py file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MedCLIP_ResNet50_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MedCLIP-ResNet50\n",
    "MedCLIP_ResNet50_model = MedCLIPModel(vision_cls=MedCLIPVisionModel).to(device)\n",
    "accuracy = medclip_zero_shot(MedCLIP_ResNet50_model, ham_train, classes)\n",
    "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MedCLIP_ViT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MedCLIP-ViT\n",
    "MedCLIP_ViT_model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT).to(device)\n",
    "accuracy = medclip_zero_shot(MedCLIP_ViT_model, ham_train, classes)\n",
    "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NIH Chest X-ray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting random image directory...\n",
      "Loading csv file from data/nih/Data_Entry_2017.csv\n",
      "Loading dataset from data/nih/images\n",
      "Loading NIH dataset... data/nih/Data_Entry_2017.csv\n",
      "Loading images from... data/nih/images\n",
      "Original Dataset length: 112120\n",
      "Train dataset length: 89696\n",
      "Test dataset length: 22424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x187c1a9b350>,\n",
       " <torch.utils.data.dataset.Subset at 0x187c1a9b2d0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import load_nih_dataset_split, NIH_CLASS_TYPES\n",
    "import torch\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT, MedCLIPVisionModel\n",
    "from build.lib.medclip.constants import BERT_TYPE, IMG_MEAN, IMG_STD, IMG_SIZE\n",
    "import torchvision\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[IMG_MEAN], std=[IMG_STD])\n",
    "])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# NIH_CLASS_TYPES\n",
    "classes = list(NIH_CLASS_TYPES)  # From the data_utils.py file\n",
    "classes\n",
    "nih_train, nih_test = load_nih_dataset_split(transform=transform)\n",
    "nih_train, nih_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medclip_zero_shot(model, test_dataset, classes, batch_size=BATCH_SIZE):\n",
    "    # Data loader for the dataset\n",
    "    data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Prepare text prompts\n",
    "    text_prompts = [f\"a photo of a {c}, a type of skin lesion.\" for c in classes]\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Tokenize text prompts and convert to tensors\n",
    "    text_tokens = [tokenizer(text, return_tensors='pt', padding=True, truncation=False, add_special_tokens=True) for text in text_prompts]\n",
    "\n",
    "    # Encode text prompts using MedClip's text model\n",
    "    # Inside the medclip_zero_shot function\n",
    "    text_features = [\n",
    "        model.encode_text(\n",
    "            input_ids=tokens['input_ids'].to(device), \n",
    "            attention_mask=tokens['attention_mask'].to(device)\n",
    "        ) \n",
    "        for tokens in text_tokens\n",
    "    ]\n",
    "\n",
    "    # Initialize variables for accuracy calculation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features_tensor = torch.cat(text_features, dim=0)\n",
    "\n",
    "        similarity = torch.matmul(image_features, text_features_tensor.t())\n",
    "        probabilities = F.sigmoid(similarity)  # Convert to probabilities\n",
    "        predictions = (probabilities > 0.5).float()  # Apply threshold to get binary predictions\n",
    "\n",
    "        # Update correct and total counts\n",
    "        # Calculate correct predictions in a multi-label scenario\n",
    "        correct_preds = (predictions == labels).all(dim=1).sum().item()\n",
    "        correct += correct_preds\n",
    "        total += len(labels)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mario\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 701/701 [13:59<00:00,  1.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy = 0.852%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load MedCLIP-ResNet50\n",
    "MedCLIP_ResNet50_model = MedCLIPModel(vision_cls=MedCLIPVisionModel).to(device)\n",
    "MedCLIP_ResNet50_model\n",
    "accuracy = medclip_zero_shot(MedCLIP_ResNet50_model, nih_train, classes)\n",
    "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\anaconda3\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 701/701 [1:01:22<00:00,  5.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy = 0.618%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load MedCLIP-ViT\n",
    "MedCLIP_ViT_model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT).to(device)\n",
    "accuracy = medclip_zero_shot(MedCLIP_ViT_model, nih_train, classes)\n",
    "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
